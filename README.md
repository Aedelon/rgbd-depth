# Camera Depth Models (CDM)

Optimized Python package for RGB-D depth refinement using Vision Transformer encoders. This implementation is aligned with the [ByteDance CDM reference implementation](https://github.com/bytedance/camera-depth-models) with additional performance optimizations for CUDA, MPS (Apple Silicon), and CPU.

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

## Overview

Camera Depth Models (CDMs) are sensor-specific depth models trained to produce clean, simulation-like depth maps from noisy real-world depth camera data. By bridging the visual gap between simulation and reality through depth perception, CDMs enable robotic policies trained purely in simulation to transfer directly to real robots.

**Original work by ByteDance Research.** This package provides an optimized implementation with:
- âœ… **Pixel-perfect alignment** with reference implementation (verified: 0 pixel difference)
- âš¡ **Device-specific optimizations**: xFormers (CUDA), SDPA fallback, torch.compile
- ðŸŽ¯ **Mixed precision support**: FP16 (CUDA/MPS), BF16 (CUDA)
- ðŸ”§ **Better CLI**: Device selection, optimization control, precision modes
- ðŸ“¦ **Easy installation**: Single `pip install` command

### Key Features

- **Metric Depth Estimation**: Produces accurate absolute depth measurements in meters
- **Multi-Camera Support**: Optimized models for various depth sensors (RealSense D405/D435/L515, ZED 2i, Azure Kinect)
- **Performance Optimizations**: ~8% faster on CUDA with xFormers, automatic backend selection
- **Mixed Precision**: FP16/BF16 support for faster inference on compatible hardware
- **Sim-to-Real Ready**: Generates simulation-quality depth from real camera data

## Architecture

CDM uses a dual-branch Vision Transformer architecture:
- **RGB Branch**: Extracts semantic information from RGB images
- **Depth Branch**: Processes noisy depth sensor data
- **Cross-Attention Fusion**: Combines RGB semantics with depth scale information
- **DPT Decoder**: Produces final metric depth estimation

Supported ViT encoder sizes:
- `vits`: Small (64 features, 384 output channels)
- `vitb`: Base (128 features, 768 output channels)
- `vitl`: Large (256 features, 1024 output channels)
- `vitg`: Giant (384 features, 1536 output channels)

All pretrained models we provide are based on `vitl`.

## Installation

### From PyPI (recommended)

```bash
# Basic installation
pip install rgbd-depth

# With CUDA optimizations (xFormers)
pip install rgbd-depth[xformers]

# Development installation
git clone https://github.com/Aedelon/camera-depth-models.git
cd camera-depth-models
pip install -e .
```

**Requirements:**
- Python 3.8+
- PyTorch 2.0+ with appropriate CUDA/MPS support
- OpenCV, NumPy, Pillow

## Quick Start

```bash
# CUDA (optimizations auto-enabled, FP16 for best speed)
python infer.py --input rgb.png --depth depth.png --precision fp16

# Apple Silicon (MPS)
python infer.py --input rgb.png --depth depth.png --device mps

# CPU (FP32 only)
python infer.py --input rgb.png --depth depth.png --device cpu
```

> Example images are provided in `input_data/`. Pre-trained models can be downloaded from [Hugging Face](https://huggingface.co/collections/depth-anything/camera-depth-models-68b521181dedd223f4b020db).

## Usage

### Command Line Interface

**Basic inference:**
```bash
python infer.py \
    --input /path/to/rgb.png \
    --depth /path/to/depth.png \
    --output refined_depth.png
```

**CUDA with optimizations (default):**
```bash
# FP32 (best accuracy)
python infer.py --input rgb.png --depth depth.png

# FP16 (best speed, ~2Ã— faster)
python infer.py --input rgb.png --depth depth.png --precision fp16

# BF16 (best stability)
python infer.py --input rgb.png --depth depth.png --precision bf16

# Disable optimizations (debugging)
python infer.py --input rgb.png --depth depth.png --no-optimize
```

**Apple Silicon (MPS):**
```bash
# FP32 (default)
python infer.py --input rgb.png --depth depth.png --device mps

# FP16 (faster)
python infer.py --input rgb.png --depth depth.png --device mps --precision fp16
```

**CPU:**
```bash
# FP32 only (FP16 not recommended on CPU)
python infer.py --input rgb.png --depth depth.png --device cpu
```

### Command Line Arguments

**Required:**
- `--input`: Path to RGB input image (JPG/PNG)
- `--depth`: Path to depth input image (PNG, 16-bit or 32-bit)

**Optional:**
- `--output`: Output visualization path (default: `output.png`)
- `--device`: Device to use: `auto`, `cuda`, `mps`, `cpu` (default: `auto`)
- `--precision`: Precision mode: `fp32`, `fp16`, `bf16` (default: `fp32`)
- `--no-optimize`: Disable optimizations on CUDA (for debugging)
- `--encoder`: Model size: `vits`, `vitb`, `vitl`, `vitg` (default: `vitl`)
- `--input-size`: Input resolution for inference (default: 518)
- `--depth-scale`: Scale factor for depth values (default: 1000.0)
- `--max-depth`: Maximum valid depth in meters (default: 6.0)

### Python API

```python
import torch
from rgbddepth.dpt import RGBDDepth
import cv2
import numpy as np

# Load model with optimizations
model = RGBDDepth(encoder='vitl', features=256, use_xformers=True)
model.load_state_dict(torch.load('model.pth'))
model.eval()
model = model.to('cuda')  # or 'mps', 'cpu'

# Optional: compile for extra speed on CUDA
model = torch.compile(model)

# Load images
rgb = cv2.imread('rgb.jpg')[:, :, ::-1]  # BGR to RGB
depth = cv2.imread('depth.png', cv2.IMREAD_UNCHANGED) / 1000.0  # Convert to meters

# Create similarity depth (inverse depth)
simi_depth = np.zeros_like(depth)
simi_depth[depth > 0] = 1 / depth[depth > 0]

# Run inference with mixed precision
with torch.amp.autocast('cuda', dtype=torch.float16):
    pred_depth = model.infer_image(rgb, simi_depth, input_size=518)
```

## Model Training

CDMs are trained on synthetic datasets generated using camera-specific noise models:

1. **Noise Model Training**: Learn hole and value noise patterns from real camera data
2. **Synthetic Data Generation**: Apply learned noise to clean simulation depth
3. **CDM Training**: Train depth estimation model on synthetic noisy data

Training datasets: HyperSim, DREDS, HISS, IRS (280,000+ images total)

## Supported Cameras

We currently provide pre-trained models available for:
- Intel RealSense D405/D435/L515
- Stereolabs ZED 2i (2 modes: Quality, Neural)
- Microsoft Azure Kinect

## File Structure

```
cdm/
â”œâ”€â”€ infer.py              # Main inference script
â”œâ”€â”€ setup.py              # Package installation
â”œâ”€â”€ rgbddepth/            # Core package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dpt.py            # Main RGBDDepth model
â”‚   â”œâ”€â”€ dinov2.py         # DINOv2 encoder
â”‚   â”œâ”€â”€ dinov2_layers/    # ViT transformer layers
â”‚   â””â”€â”€ util/             # Utility functions
â”‚       â”œâ”€â”€ blocks.py     # Neural network blocks
â”‚       â””â”€â”€ transform.py  # Image preprocessing
â””â”€â”€ README.md
```

## Performance

### Accuracy

This implementation achieves **pixel-perfect alignment** with the ByteDance reference:
- âœ… **0 pixel difference** between vanilla and optimized inference (verified on test images)
- âœ… **Identical checkpoint loading** (weights are fully compatible)
- âœ… **Numerical precision preserved** (min=0.2036, max=1.1217, exact match)

CDMs achieve state-of-the-art performance on metric depth estimation:
- Superior accuracy compared to existing prompt-based depth models
- Zero-shot generalization across different camera types
- Real-time inference suitable for robot control (lightweight ViT variants)

### Speed Benchmarks

| Device | Mode | Precision | Time | vs Baseline | Notes |
|--------|------|-----------|------|-------------|-------|
| **CUDA** | Vanilla | FP32 | TBD | - | Reference |
| **CUDA** | Optimized (xFormers) | FP32 | TBD | ~8% faster | Recommended |
| **CUDA** | Optimized | FP16 | TBD | ~2Ã— faster | Best speed |
| **CUDA** | Optimized | BF16 | TBD | ~2Ã— faster | Best stability |
| **MPS** | Vanilla | FP32 | 1.34s | - | torch.compile: no gain |
| **MPS** | Vanilla | FP16 | TBD | TBD | To be benchmarked |
| **CPU** | Vanilla | FP32 | 13.37s | - | Optimizations: -11% slower |

**Notes:**
- **CUDA**: Optimizations auto-enabled by default (use `--no-optimize` to disable)
- **MPS**: torch.compile provides no gain for Vision Transformers (~0% improvement)
- **CPU**: torch.compile is counterproductive (compilation overhead > gains)
- xFormers is CUDA-only (~8% faster than native SDPA)

For detailed optimization strategies, see [OPTIMIZATION.md](OPTIMIZATION.md).

## What's Different from Reference?

This implementation maintains **100% compatibility** with ByteDance CDM while adding:

### 1. Performance Optimizations
- **xFormers support**: ~8% faster attention on CUDA (automatic fallback to SDPA)
- **torch.compile**: JIT compilation (CUDA only, auto-enabled)
- **Mixed precision**: FP16/BF16 support via `torch.amp.autocast`
- **Device-specific strategies**: Optimizations only where beneficial

### 2. Better CLI/API
- `--device` flag: Force specific device (auto/cuda/mps/cpu)
- `--precision` flag: Choose FP32/FP16/BF16
- `--no-optimize` flag: Disable optimizations for debugging
- Automatic device detection and optimization selection

### 3. Improved Architecture
- `FlexibleCrossAttention`: Inherits from `nn.MultiheadAttention` for checkpoint compatibility
- Automatic backend selection: xFormers (CUDA) â†’ SDPA (fallback)
- Device-aware preprocessing: Uses model's device instead of auto-detection

### 4. Code Quality
- Type hints and better documentation
- Cleaner argument parsing
- Validation for precision/device combinations
- Helpful warnings for incompatible configurations

All changes are **backwards compatible** with original checkpoints and produce **identical numerical results**.

## Citation

If you use CDM in your research, please cite:

```bibtex
@article{liu2025manipulation,
  title={Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots},
  author={Liu, Minghuan and Zhu, Zhengbang and Han, Xiaoshen and Hu, Peng and Lin, Haotong and
          Li, Xinyao and Chen, Jingxiao and Xu, Jiafeng and Yang, Yichu and Lin, Yunfeng and
          Li, Xinghang and Yu, Yong and Zhang, Weinan and Kong, Tao and Kang, Bingyi},
  journal={arXiv preprint},
  year={2025}
}
```

## License

This project is licensed under the Apache 2.0 License. See [LICENSE](../LICENSE) for details.
